"""Bitmask operations: allocate, apply, etc."""

import math
from typing import List, Optional, Tuple

import torch

bitmask_dtype = torch.int32
"""The dtype of the bitmask: int32."""


def get_bitmask_shape(batch_size: int, vocab_size: int) -> Tuple[int, int]:
    """Return the shape of the bitmask: (batch_size, ceil(vocab_size / 32))."""
    return (batch_size, math.ceil(vocab_size / 32))


_FULL_MASK = torch.tensor(-1, dtype=bitmask_dtype)


def allocate_token_bitmask(batch_size: int, vocab_size: int) -> torch.Tensor:
    """Allocate the bitmask for the next token prediction. The bitmask is an int32 tensor on
    CPU with shape (batch_size, ceil(vocab_size / 32)). Users who have their own needs to
    manage CUDA memory can construct the tensor with get_bitmask_shape and bitmask_dtype
    themselves.

    The reason why we use int32 instead of uint32 is that old versions of PyTorch do not support
    uint32.

    Parameters
    ----------
    batch_size : int
        The batch size of the bitmask.

    vocab_size : int
        The size of the vocabulary.

    Returns
    -------
    bitmask : torch.Tensor
        The shape of the bitmask.
    """
    # In CUDA, use pinned memory to speed up data transfer from CPU to GPU
    return torch.full(get_bitmask_shape(batch_size, vocab_size), _FULL_MASK, dtype=bitmask_dtype)


def reset_token_bitmask(bitmask: torch.Tensor) -> None:
    """Reset the bitmask to the full mask."""
    bitmask.fill_(_FULL_MASK)


def apply_token_bitmask_inplace(
    logits: torch.Tensor,
    bitmask: torch.Tensor,
    *,
    vocab_size: Optional[int] = None,
    indices: Optional[List[int]] = None,
) -> None:
    """Apply the bitmask to the logits in-place. The bitmask is a 01 bitwise compressed tensor,
    where 0 means the token is masked and 1 means the token is not masked. It can be generated by
    allocate_token_bitmask and filled by fill_next_token_bitmask. After applying the bitmask, the
    masked logits will be set to -inf.

    The shape of logits and bitmask should be (batch_size, vocab_size) and
    (batch_size, bitmask_size) respectively. bitmask_size = ceil(vocab_size / 32). The operation is:

    .. code:: python

        for i in range(batch_size):
            for j in range(vocab_size):
                if get_bitmask_value(bitmask, i, j) == 0:
                    logits[i, j] = -inf

    get_bitmask_value(bitmask, i, j) gets the j-th bit of the i-th row of the bitmask.

    Notes
    -----
    Padding:
        This method allows additional padding on the vocabulary dimension of logits or bitmask. If
        padding exists, provide the real vocab size to the vocab_size parameter, and the operation
        will be applied to logits[..., :vocab_size] and bitmask[..., :ceil(vocab_size / 32)].

        If vocab_size is not provided, the vocab size will be detected as min(logits.shape[-1],
        bitmask.shape[-1] * 32).

    Indices:
        Indices can be used to specify which logits in the batch to apply the bitmask to. It is
        especially useful when there are structured requests and unstructured requests mixed in the
        same batch by skipping masking the logits in the unstructured requests. When specified, the
        operation will be

        .. code:: python

            for batch_id in indices:
                for j in range(vocab_size):
                    if get_bitmask_value(bitmask, batch_id, j) == 0:
                        logits[batch_id, j] = -inf

        When indices is specified, the batch sizes of logits and bitmask do not need to be the same.
        As long as the indices are valid, the operation will be performed.

    Device:
        The logits and bitmask should be on the same device. If both them are on GPU, we launch a GPU
        kernel to apply bitmask. If both them are on CPU, we use a CPU implementation. The GPU kernel
        is optimized and should be preferred.

        In practice, the bitmask is allocated on CPU, and the logits is usually on GPU, so users should
        manually copy the bitmask to GPU before calling this function.

    Parameters
    ----------
    logits : torch.Tensor
        The tensor to apply the bitmask to.

    bitmask : torch.Tensor
        The bitmask to apply.

    vocab_size : Optional[int], default: None
        The size of the vocabulary. If not provided, the vocab size will be detected as
        min(logits.shape[-1], bitmask.shape[-1] * 32).

    indices : Optional[List[int]], default: None
        A list of indices to specify which logits in the batch to apply the bitmask to. Should be
        unique. If None, apply the bitmask to all logits in the batch.
    """
    if bitmask.device != logits.device:
        raise ValueError(
            "logits and bitmask should be on the same device. "
            + f"But got logits.device: {logits.device}, bitmask.device: {bitmask.device}"
        )

    # dispatch to different implementations based on the device
    if logits.device.type == "cpu":
        from .kernels.apply_token_bitmask_inplace_cpu import apply_token_bitmask_inplace_cpu

        apply_token_bitmask_inplace_cpu(logits, bitmask, vocab_size, indices)

    elif logits.device.type == "cuda":
        from .kernels.apply_token_bitmask_inplace_triton import apply_token_bitmask_inplace_triton

        apply_token_bitmask_inplace_triton(logits, bitmask, vocab_size, indices)
    else:
        from .kernels.apply_token_bitmask_inplace_torch_compile import (
            apply_token_bitmask_inplace_torch_compile,
        )

        apply_token_bitmask_inplace_torch_compile(logits, bitmask, vocab_size, indices)


def bitmask_to_bool_tensor(bitmask: torch.Tensor, vocab_size: int) -> torch.Tensor:
    """Get the bool tensor from a bitmask. 1 means the token is allowed, while 0 means the token is
    masked. This function is mainly for debug purpose.

    Bitmask can be a 1D tensor with shape (bitmask_size,) or a 2D tensor with shape
    (batch_size, bitmask_size). It will unpack the bitmask into a bool tensor with shape
    (bitmask_size * 32,) or (batch_size, bitmask_size * 32), and then the first vocab_size
    positions will be kept.

    Parameters
    ----------
    bitmask : torch.Tensor
        The bitmask.

    vocab_size : int
        The size of the vocabulary.

    Returns
    -------
    bool_tensor : torch.Tensor
        The bool tensor.
    """
    from .kernels.bitmask_to_bool_tensor_torch_compile import bitmask_to_bool_tensor_kernel

    return bitmask_to_bool_tensor_kernel(bitmask, vocab_size)


def bool_tensor_to_bitmask(bool_tensor: torch.Tensor) -> torch.Tensor:
    """Get the bitmask from a bool tensor. True means the token is allowed, while False means the
    token is rejected. This function is mainly for debug purpose.

    Parameters
    ----------
    bool_tensor : torch.Tensor
        The bool tensor. The shape is (batch_size, vocab_size) or (vocab_size,).

    Returns
    -------
    bitmask : torch.Tensor
        The bitmask. The shape is (batch_size, ceil(vocab_size / 32)) or
        (ceil(vocab_size / 32),) respectively.
    """
    from .kernels.bitmask_to_bool_tensor_torch_compile import bool_tensor_to_bitmask_kernel

    return bool_tensor_to_bitmask_kernel(bool_tensor)
